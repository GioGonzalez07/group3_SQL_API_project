{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#import sql_functions # as sf\n",
    "\n",
    "import sqlalchemy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# querystring has to be changed to the right cities!!\n",
    "querystring = {\"station\":\"10637\",\"start\":\"2020-01-01\",\"end\":\"2020-01-05\"}\n",
    "\n",
    "headers = {\n",
    "\t\"X-RapidAPI-Key\": os.getenv('api_key'),\n",
    "\t\"X-RapidAPI-Host\": \"meteostat.p.rapidapi.com\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meta': {'generated': '2024-05-08 09:38:13'}, 'data': [{'date': '2020-01-01', 'tavg': 0.4, 'tmin': -3.6, 'tmax': 4.5, 'prcp': 0.1, 'snow': 0.0, 'wdir': 47.0, 'wspd': 10.1, 'wpgt': 22.3, 'pres': 1035.5, 'tsun': 438}, {'date': '2020-01-02', 'tavg': -0.5, 'tmin': -2.4, 'tmax': 1.4, 'prcp': 0.0, 'snow': 0.0, 'wdir': 198.0, 'wspd': 6.1, 'wpgt': 25.9, 'pres': 1031.2, 'tsun': 0}, {'date': '2020-01-03', 'tavg': 5.8, 'tmin': 1.3, 'tmax': 9.3, 'prcp': 0.5, 'snow': 0.0, 'wdir': 209.0, 'wspd': 15.5, 'wpgt': 46.8, 'pres': 1024.4, 'tsun': 0}, {'date': '2020-01-04', 'tavg': 5.4, 'tmin': 2.4, 'tmax': 6.8, 'prcp': 0.2, 'snow': 0.0, 'wdir': 242.0, 'wspd': 15.8, 'wpgt': 47.5, 'pres': 1031.5, 'tsun': 0}, {'date': '2020-01-05', 'tavg': 4.8, 'tmin': 2.7, 'tmax': 6.3, 'prcp': 1.1, 'snow': 0.0, 'wdir': 189.0, 'wspd': 6.8, 'wpgt': 23.4, 'pres': 1036.6, 'tsun': 0}]}\n"
     ]
    }
   ],
   "source": [
    "url_daily = \"https://meteostat.p.rapidapi.com/stations/daily\"\n",
    "response_daily = requests.get(url_daily, headers=headers, params=querystring)\n",
    "weather_daily = response_daily.json()\n",
    "\n",
    "print(weather_daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n   \"meta\": {\\n      \"generated\": \"2024-05-08 09:38:13\"\\n   },\\n   \"data\": [\\n      {\\n         \"date\": \"2020-01-01\",\\n         \"tavg\": 0.4,\\n         \"tmin\": -3.6,\\n         \"tmax\": 4.5,\\n         \"prcp\": 0.1,\\n         \"snow\": 0.0,\\n         \"wdir\": 47.0,\\n         \"wspd\": 10.1,\\n         \"wpgt\": 22.3,\\n         \"pres\": 1035.5,\\n         \"tsun\": 438\\n      },\\n      {\\n         \"date\": \"2020-01-02\",\\n         \"tavg\": -0.5,\\n         \"tmin\": -2.4,\\n         \"tmax\": 1.4,\\n         \"prcp\": 0.0,\\n         \"snow\": 0.0,\\n         \"wdir\": 198.0,\\n         \"wspd\": 6.1,\\n         \"wpgt\": 25.9,\\n         \"pres\": 1031.2,\\n         \"tsun\": 0\\n      },\\n      {\\n         \"date\": \"2020-01-03\",\\n         \"tavg\": 5.8,\\n         \"tmin\": 1.3,\\n         \"tmax\": 9.3,\\n         \"prcp\": 0.5,\\n         \"snow\": 0.0,\\n         \"wdir\": 209.0,\\n         \"wspd\": 15.5,\\n         \"wpgt\": 46.8,\\n         \"pres\": 1024.4,\\n         \"tsun\": 0\\n      },\\n      {\\n         \"date\": \"2020-01-04\",\\n         \"tavg\": 5.4,\\n         \"tmin\": 2.4,\\n         \"tmax\": 6.8,\\n         \"prcp\": 0.2,\\n         \"snow\": 0.0,\\n         \"wdir\": 242.0,\\n         \"wspd\": 15.8,\\n         \"wpgt\": 47.5,\\n         \"pres\": 1031.5,\\n         \"tsun\": 0\\n      },\\n      {\\n         \"date\": \"2020-01-05\",\\n         \"tavg\": 4.8,\\n         \"tmin\": 2.7,\\n         \"tmax\": 6.3,\\n         \"prcp\": 1.1,\\n         \"snow\": 0.0,\\n         \"wdir\": 189.0,\\n         \"wspd\": 6.8,\\n         \"wpgt\": 23.4,\\n         \"pres\": 1036.6,\\n         \"tsun\": 0\\n      }\\n   ]\\n}'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create json object: json_object\n",
    "json_object = json.loads(response_daily.content)\n",
    "\n",
    "# Print json_object\n",
    "json.dumps(json_object, indent = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta: {'generated': '2024-05-08 09:38:13'}\n",
      "data: [{'date': '2020-01-01', 'tavg': 0.4, 'tmin': -3.6, 'tmax': 4.5, 'prcp': 0.1, 'snow': 0.0, 'wdir': 47.0, 'wspd': 10.1, 'wpgt': 22.3, 'pres': 1035.5, 'tsun': 438}, {'date': '2020-01-02', 'tavg': -0.5, 'tmin': -2.4, 'tmax': 1.4, 'prcp': 0.0, 'snow': 0.0, 'wdir': 198.0, 'wspd': 6.1, 'wpgt': 25.9, 'pres': 1031.2, 'tsun': 0}, {'date': '2020-01-03', 'tavg': 5.8, 'tmin': 1.3, 'tmax': 9.3, 'prcp': 0.5, 'snow': 0.0, 'wdir': 209.0, 'wspd': 15.5, 'wpgt': 46.8, 'pres': 1024.4, 'tsun': 0}, {'date': '2020-01-04', 'tavg': 5.4, 'tmin': 2.4, 'tmax': 6.8, 'prcp': 0.2, 'snow': 0.0, 'wdir': 242.0, 'wspd': 15.8, 'wpgt': 47.5, 'pres': 1031.5, 'tsun': 0}, {'date': '2020-01-05', 'tavg': 4.8, 'tmin': 2.7, 'tmax': 6.3, 'prcp': 1.1, 'snow': 0.0, 'wdir': 189.0, 'wspd': 6.8, 'wpgt': 23.4, 'pres': 1036.6, 'tsun': 0}]\n"
     ]
    }
   ],
   "source": [
    "for key, value in weather_daily.items():\n",
    "    print(key + ':', value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weather_date</th>\n",
       "      <th>weather_tavg</th>\n",
       "      <th>weather_tmin</th>\n",
       "      <th>weather_tmax</th>\n",
       "      <th>weather_prcp</th>\n",
       "      <th>weather_snow</th>\n",
       "      <th>weather_wdir</th>\n",
       "      <th>weather_wspd</th>\n",
       "      <th>weather_wpgt</th>\n",
       "      <th>weather_pres</th>\n",
       "      <th>weather_tsun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-3.6</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>10.1</td>\n",
       "      <td>22.3</td>\n",
       "      <td>1035.5</td>\n",
       "      <td>438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-2.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>25.9</td>\n",
       "      <td>1031.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.3</td>\n",
       "      <td>9.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>15.5</td>\n",
       "      <td>46.8</td>\n",
       "      <td>1024.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.4</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>15.8</td>\n",
       "      <td>47.5</td>\n",
       "      <td>1031.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>4.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>6.3</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>23.4</td>\n",
       "      <td>1036.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  weather_date  weather_tavg  weather_tmin  weather_tmax  weather_prcp   \n",
       "0   2020-01-01           0.4          -3.6           4.5           0.1  \\\n",
       "1   2020-01-02          -0.5          -2.4           1.4           0.0   \n",
       "2   2020-01-03           5.8           1.3           9.3           0.5   \n",
       "3   2020-01-04           5.4           2.4           6.8           0.2   \n",
       "4   2020-01-05           4.8           2.7           6.3           1.1   \n",
       "\n",
       "   weather_snow  weather_wdir  weather_wspd  weather_wpgt  weather_pres   \n",
       "0           0.0          47.0          10.1          22.3        1035.5  \\\n",
       "1           0.0         198.0           6.1          25.9        1031.2   \n",
       "2           0.0         209.0          15.5          46.8        1024.4   \n",
       "3           0.0         242.0          15.8          47.5        1031.5   \n",
       "4           0.0         189.0           6.8          23.4        1036.6   \n",
       "\n",
       "   weather_tsun  \n",
       "0           438  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_daily_df = pd.json_normalize(weather_daily,\n",
    "                                      #sep='_',\n",
    "                                      record_path='data',\n",
    "                                      #meta=[['generated']],\n",
    "                                      record_prefix='weather_'\n",
    "                                      )\n",
    "\n",
    "weather_daily_real_df = pd.DataFrame(weather_daily_df)\n",
    "weather_daily_real_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'data' not found. If specifying a record_path, all elements of data should have the path.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/nf_sql/lib/python3.9/site-packages/pandas/io/json/_normalize.py:400\u001b[0m, in \u001b[0;36mjson_normalize.<locals>._pull_field\u001b[0;34m(js, spec, extract_record)\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 400\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'data'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m weather_temp \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Flatten json response\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m weather_temp_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson_normalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweather_temp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                            \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mrecord_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mrecord_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweather_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     58\u001b[0m \u001b[43m                            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# concatenate dataframes\u001b[39;00m\n\u001b[1;32m     60\u001b[0m weather_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([weather_df, weather_temp_df], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/nf_sql/lib/python3.9/site-packages/pandas/io/json/_normalize.py:519\u001b[0m, in \u001b[0;36mjson_normalize\u001b[0;34m(data, record_path, meta, meta_prefix, record_prefix, errors, sep, max_level)\u001b[0m\n\u001b[1;32m    516\u001b[0m                 meta_vals[key]\u001b[38;5;241m.\u001b[39mappend(meta_val)\n\u001b[1;32m    517\u001b[0m             records\u001b[38;5;241m.\u001b[39mextend(recs)\n\u001b[0;32m--> 519\u001b[0m \u001b[43m_recursive_extract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecord_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m result \u001b[38;5;241m=\u001b[39m DataFrame(records)\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m record_prefix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/nf_sql/lib/python3.9/site-packages/pandas/io/json/_normalize.py:501\u001b[0m, in \u001b[0;36mjson_normalize.<locals>._recursive_extract\u001b[0;34m(data, path, seen_meta, level)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m--> 501\u001b[0m         recs \u001b[38;5;241m=\u001b[39m \u001b[43m_pull_records\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m         recs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    503\u001b[0m             nested_to_record(r, sep\u001b[38;5;241m=\u001b[39msep, max_level\u001b[38;5;241m=\u001b[39mmax_level)\n\u001b[1;32m    504\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(r, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m    505\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m r\n\u001b[1;32m    506\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m recs\n\u001b[1;32m    507\u001b[0m         ]\n\u001b[1;32m    509\u001b[0m         \u001b[38;5;66;03m# For repeating the metadata later\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/nf_sql/lib/python3.9/site-packages/pandas/io/json/_normalize.py:423\u001b[0m, in \u001b[0;36mjson_normalize.<locals>._pull_records\u001b[0;34m(js, spec)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_pull_records\u001b[39m(js: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], spec: \u001b[38;5;28mlist\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    418\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;124;03m    Internal function to pull field for records, and similar to\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;124;03m    _pull_field, but require to return list. And will raise error\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;124;03m    if has non iterable value.\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 423\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_pull_field\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextract_record\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;66;03m# GH 31507 GH 30145, GH 26284 if result is not list, raise TypeError if not\u001b[39;00m\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;66;03m# null, otherwise return an empty list\u001b[39;00m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mlist\u001b[39m):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/nf_sql/lib/python3.9/site-packages/pandas/io/json/_normalize.py:403\u001b[0m, in \u001b[0;36mjson_normalize.<locals>._pull_field\u001b[0;34m(js, spec, extract_record)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m extract_record:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m    404\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found. If specifying a record_path, all elements of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    405\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata should have the path.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    406\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    408\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'data' not found. If specifying a record_path, all elements of data should have the path.\""
     ]
    }
   ],
   "source": [
    "# Set the url\n",
    "url = \"https://meteostat.p.rapidapi.com/stations/daily\"\n",
    "\n",
    "# Settings:\n",
    "stations = [        # translated into station codes\n",
    "            '74486',            # New York - John F. Kennedy Airport\n",
    "#            '72509',            # Boston Logan International Airport\n",
    "#            '72405',            # Washington D.C. National Airport\n",
    "#            '72502',            # New Jersey - Newark Airport\n",
    "#            '72202',            #'Miami International Airport'\n",
    "#            '72243',            #'Houston, TX Intercontinental'\n",
    "#            '72494',            #'San Francisco Airport'\n",
    "            '72793'             #'Seattle-Tacoma Airport'\n",
    "            ]\n",
    "\n",
    "start_dates = [\n",
    "                '2015-01-01',\n",
    "#               '2015-12-01',\n",
    "#                '2016-12-01',\n",
    "                '2017-12-01'\n",
    "                ]\n",
    "\n",
    "\n",
    "end_dates = [\n",
    "            '2015-02-28',\n",
    "#            '2016-02-29',\n",
    "#            '2017-02-28',\n",
    "            '2017-12-31'\n",
    "            ]\n",
    "\n",
    "headers = {\n",
    "\t\"X-RapidAPI-Key\": os.getenv('api_key'),\n",
    "\t\"X-RapidAPI-Host\": \"meteostat.p.rapidapi.com\"\n",
    "    }\n",
    "\n",
    "# Create empty dataframe, will be used to append each locations and times weather data\n",
    "weather_df = pd.DataFrame([])\n",
    "\n",
    "# Loop through all locations and all dates\n",
    "for station in stations:\n",
    "    for start_date, end_date in zip(start_dates, end_dates):\n",
    "\n",
    "        querystring = {\"station\":{station},\"start\":{start_date},\"end\":{end_date}, \"units\":\"metric\"}\n",
    "\n",
    "        # Request data from url\n",
    "        r = requests.get(url, headers=headers, params=querystring)\n",
    "\n",
    "        # time.sleep(1) #uncomment if you run into a query limit\n",
    "\n",
    "        # Decode response with json decoder\n",
    "        weather_temp = r.json()\n",
    "\n",
    "        # Flatten json response\n",
    "        weather_temp_df = pd.json_normalize(weather_temp,\n",
    "                                    sep='_',\n",
    "                                    record_path='data',\n",
    "                                    record_prefix='weather_'\n",
    "                                    )\n",
    "        # concatenate dataframes\n",
    "        weather_df = pd.concat([weather_df, weather_temp_df], ignore_index=True)\n",
    "\n",
    "    #weather_df['station'] = station\n",
    "\n",
    "    # Set the station code for all rows related to the current date range\n",
    "    #weather_df.loc[weather_df['weather_date'] >= start_date, 'station'] = station\n",
    "\n",
    "# Print final dataset weather_df\n",
    "weather_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['72793'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_df['station'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = [        # translated into station codes\n",
    "            '74486',            # New York - John F. Kennedy Airport\n",
    "            '72509' #,            # Boston Logan International Airport\n",
    "#            '72405',            # Washington D.C. National Airport\n",
    "#            '72502',            # New Jersey - Newark Airport\n",
    "#            '72202',            #'Miami International Airport'\n",
    "#            '72243',            #'Houston, TX Intercontinental'\n",
    "#            '72494',            #'San Francisco Airport'\n",
    "#            '72793'            #'Seattle-Tacoma Airport'\n",
    "            ]\n",
    "\n",
    "\n",
    "start_dates = [\n",
    "                '2015-01-01',\n",
    "                '2015-12-01',\n",
    "                '2016-12-01',\n",
    "                '2017-12-01'\n",
    "                ]\n",
    "\n",
    "\n",
    "end_dates = [\n",
    "            '2015-02-28',\n",
    "            '2016-02-29',\n",
    "            '2017-02-28',\n",
    "            '2017-12-31'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'station': {'74486'}, 'start': {'2015-01-01'}, 'end': {'2015-02-28'}, 'units': 'metric'}\n",
      "{'station': {'74486'}, 'start': {'2015-12-01'}, 'end': {'2016-02-29'}, 'units': 'metric'}\n",
      "{'station': {'74486'}, 'start': {'2016-12-01'}, 'end': {'2017-02-28'}, 'units': 'metric'}\n",
      "{'station': {'74486'}, 'start': {'2017-12-01'}, 'end': {'2017-12-31'}, 'units': 'metric'}\n",
      "{'station': {'72509'}, 'start': {'2015-01-01'}, 'end': {'2015-02-28'}, 'units': 'metric'}\n",
      "{'station': {'72509'}, 'start': {'2015-12-01'}, 'end': {'2016-02-29'}, 'units': 'metric'}\n",
      "{'station': {'72509'}, 'start': {'2016-12-01'}, 'end': {'2017-02-28'}, 'units': 'metric'}\n",
      "{'station': {'72509'}, 'start': {'2017-12-01'}, 'end': {'2017-12-31'}, 'units': 'metric'}\n"
     ]
    }
   ],
   "source": [
    "# sorted for stations/cities and then dates\n",
    "for station in stations:\n",
    "    for start_date, end_date in zip(start_dates, end_dates):\n",
    "        querystring = {\"station\":{station},\"start\":{start_date},\"end\":{end_date}, \"units\":\"metric\"}\n",
    "        print(querystring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted for dates and then stations/cities\n",
    "for start_date, end_date in zip(start_dates, end_dates):\n",
    "    for station in stations:\n",
    "        querystring = {\"station\":{station},\"start\":{start_date},\"end\":{end_date}, \"units\":\"metric\"}\n",
    "        print(querystring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the url\n",
    "url = \"https://meteostat.p.rapidapi.com/stations/daily\"\n",
    "\n",
    "# Settings:\n",
    "stations = [        # translated into station codes\n",
    "            '74486',            # New York - John F. Kennedy Airport\n",
    "            '72509',            # Boston Logan International Airport\n",
    "            '72405',            # Washington D.C. National Airport\n",
    "            '72502',            # New Jersey - Newark Airport\n",
    "            '72202',            #'Miami International Airport'\n",
    "            '72243',            #'Houston, TX Intercontinental'\n",
    "            '72494',            #'San Francisco Airport'\n",
    "            '72793'            #'Seattle-Tacoma Airport'\n",
    "            ]\n",
    "\n",
    "start_dates = [\n",
    "                '2015-01-01',\n",
    "                '2015-12-01',\n",
    "                '2016-12-01',\n",
    "                '2017-12-01'\n",
    "                ]\n",
    "\n",
    "\n",
    "end_dates = [\n",
    "            '2015-02-28',\n",
    "            '2016-02-29',\n",
    "            '2017-02-28',\n",
    "            '2017-12-31'\n",
    "            ]\n",
    "\n",
    "\n",
    "querystring = {\"station\":{station},\"start\":{start_date},\"end\":{end_date}, \"units\":\"metric\"}\n",
    "\n",
    "headers = {\n",
    "\t\"X-RapidAPI-Key\": os.getenv('api_key'),\n",
    "\t\"X-RapidAPI-Host\": \"meteostat.p.rapidapi.com\"\n",
    "    }\n",
    "\n",
    "# Create empty dataframe, will be used to append each locations and times weather data\n",
    "weather_df = pd.DataFrame([])\n",
    "\n",
    "\n",
    "# sorted for dates and then stations/cities\n",
    "# Loop through certain\n",
    "for start_date, end_date in zip(start_dates, end_dates):\n",
    "    for station in stations:\n",
    "        querystring = {\"station\":{station},\"start\":{start_date},\"end\":{end_date}, \"units\":\"metric\"}\n",
    "        print(querystring)\n",
    "\n",
    "\n",
    "\n",
    "# Loop through all locations\n",
    "for station in stations:\n",
    "    for start_date, end_date in start_dates, end_dates:\n",
    "\n",
    "    # Request data from url\n",
    "    r = requests.get(url, headers=headers, params=querystring)\n",
    "\n",
    "    # time.sleep(1) #uncomment if you run into a query limit\n",
    "\n",
    "    # Decode response with json decoder\n",
    "    weather_temp = r.json()\n",
    "\n",
    "    # Flatten json response\n",
    "    weather_temp_df = pd.json_normalize(weather_temp,\n",
    "                                    sep='_',\n",
    "                                    record_path='data',\n",
    "                                    record_prefix='weather_'\n",
    "                                    )\n",
    "    # concatenate dataframes\n",
    "    weather_df = pd.concat([weather_df, weather_temp_df], ignore_index=True)\n",
    "\n",
    "# Print final dataset weather_df\n",
    "weather_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COPY before creating nested loop\n",
    "\n",
    "\n",
    "# Set the url\n",
    "url = 'https://meteostat.p.rapidapi.com/stations/'\n",
    "\n",
    "# Settings:\n",
    "time = 'daily'\n",
    "stations = [        # translated into station codes\n",
    "            '74486',            # New York - John F. Kennedy Airport\n",
    "            '72509',            # Boston Logan International Airport\n",
    "            '72405',            # Washington D.C. National Airport\n",
    "            '72502',            # New Jersey - Newark Airport\n",
    "            '72202',            #'Miami International Airport'\n",
    "            '',            #'Houston, TX'\n",
    "            '',            #'San Francisco'\n",
    "            ''            #'Seattle'\n",
    "            ]\n",
    "\n",
    "start_dates = [\n",
    "                '2015-01-01',\n",
    "                '2015-12-01',\n",
    "                '2016-12-01',\n",
    "                '2017-12-01'\n",
    "                ]\n",
    "\n",
    "\n",
    "end_dates = [\n",
    "            '2015-02-28',\n",
    "            '2016-02-29',\n",
    "            '2017-02-28',\n",
    "            '2017-12-31'\n",
    "            ]\n",
    "\n",
    "\n",
    "querystring = {\"station\":{station},\"start\":{start_date},\"end\":{end_date}, \"units\":\"metric\"}\n",
    "\n",
    "headers = {\n",
    "\t\"X-RapidAPI-Key\": os.getenv('api_key'),\n",
    "\t\"X-RapidAPI-Host\": \"meteostat.p.rapidapi.com\"\n",
    "    }\n",
    "\n",
    "# Create empty dataframe, will be used to append each locations and times weather data\n",
    "weather_df = pd.DataFrame([])\n",
    "\n",
    "# Loop through all locations\n",
    "for station in stations:\n",
    "    # Create final url\n",
    "    url_f = url + time\n",
    "\n",
    "    # Request data from url\n",
    "    r = requests.get(url_f, headers=headers, params=querystring)\n",
    "\n",
    "    # time.sleep(1) #uncomment if you run into a query limit\n",
    "\n",
    "    # Decode response with json decoder\n",
    "    weather_temp = r.json()\n",
    "\n",
    "    # Flatten json response\n",
    "    weather_temp_df = pd.json_normalize(weather_temp,\n",
    "                                    sep='_',\n",
    "                                    record_path='data',\n",
    "                                    record_prefix='weather_'\n",
    "                                    )\n",
    "    # concatenate dataframes\n",
    "    weather_df = pd.concat([weather_df, weather_temp_df], ignore_index=True)\n",
    "\n",
    "# Print final dataset weather_df\n",
    "weather_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Don't let the next cell run, because I just copy/pasted the cell from the other notebook, but didn't exactly look over the code**\n",
    "\n",
    "**And I also think we just need to do this part once!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import get_engine function from sql_functions.py and set it to a variable called engine\n",
    "from sql_functions import get_engine\n",
    "engine = get_engine()\n",
    "# Import psycopg2\n",
    "import psycopg2\n",
    "\n",
    "# Write records stored in a dataframe to SQL database\n",
    "if engine!=None:\n",
    "    try:\n",
    "        weather_monthly_df.to_sql(name='weather_monthly', # Name of SQL table\n",
    "                        con=engine, # Engine or connection\n",
    "                        if_exists='replace', # Drop the table before inserting new values\n",
    "                        schema='hh_analytics_24_1', # your class schema\n",
    "                        index=False, # Write DataFrame index as a column\n",
    "                        chunksize=5000, # Specify the number of rows in each batch to be written at a time\n",
    "                        method='multi') # Pass multiple values in a single INSERT clause\n",
    "        print(f\"The weather_info table was imported successfully.\")\n",
    "    # Error handling\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "        engine = None\n",
    "else:\n",
    "    print('oups!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nf_sql",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
